# MongoDB Backup CronJob
#
# Runs mongodump daily at 3:00 AM against the replica set.
# Reads from a secondary (--readPreference=secondary) to avoid
# impacting the primary node.
#
# Backups are stored in a PVC at /backups/ with timestamped filenames.
# Old backups beyond RETENTION_DAYS are auto-cleaned.
#
# To upload to S3, uncomment the s3-upload container and create
# the aws-s3-credentials Secret (see below).
#
# Apply with:
#   kubectl apply -f databases/mongodb/backup-cronjob.yaml
#
# Manual trigger:
#   kubectl create job --from=cronjob/mongodb-backup mongodb-backup-manual -n databases
#
# Check backup logs:
#   kubectl logs -n databases -l job-name=mongodb-backup-manual
#
# List backups:
#   kubectl exec -it deploy/mongo-express -n databases -- ls -lh /backups/
#
# Restore backup:
# kubectl exec -it mongodb-0 -n databases -- mongorestore \
#  --uri="CONNECTION_STRING" \
#  --archive=/path/to/mongodb_20260220_030000.gz \
#  --gzip

---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: mongodb-backups
  namespace: databases
spec:
  accessModes: ["ReadWriteOnce"]
  resources:
    requests:
      storage: 20Gi

---
apiVersion: batch/v1
kind: CronJob
metadata:
  name: mongodb-backup
  namespace: databases
spec:
  schedule: "0 3 * * *"
  concurrencyPolicy: Forbid
  successfulJobsHistoryLimit: 3
  failedJobsHistoryLimit: 3
  jobTemplate:
    spec:
      backoffLimit: 2
      activeDeadlineSeconds: 3600
      template:
        spec:
          restartPolicy: OnFailure
          containers:
            - name: mongodump
              image: mongo:8.0
              command:
                - /bin/sh
                - -c
                - |
                  set -e
                  TIMESTAMP=$(date +%Y%m%d_%H%M%S)
                  BACKUP_FILE="/backups/mongodb_${TIMESTAMP}.gz"

                  echo "Starting backup at $(date)"
                  echo "Target: ${MONGODB_URI}"

                  mongodump \
                    --uri="${MONGODB_URI}" \
                    --readPreference=secondary \
                    --archive="${BACKUP_FILE}" \
                    --gzip

                  FILESIZE=$(du -h "${BACKUP_FILE}" | cut -f1)
                  echo "Backup complete: ${BACKUP_FILE} (${FILESIZE})"

                  echo "Cleaning backups older than ${RETENTION_DAYS} days..."
                  find /backups -name "mongodb_*.gz" -mtime +${RETENTION_DAYS} -delete

                  echo "Current backups:"
                  ls -lh /backups/mongodb_*.gz

                  echo "Backup finished at $(date)"
              env:
                - name: MONGODB_URI
                  valueFrom:
                    secretKeyRef:
                      name: mongodb-admin-root
                      key: connectionString.standard
                - name: RETENTION_DAYS
                  value: "7"
              resources:
                requests:
                  cpu: 100m
                  memory: 256Mi
                limits:
                  cpu: 500m
                  memory: 512Mi
              volumeMounts:
                - name: backup-storage
                  mountPath: /backups

            # --- Uncomment to upload backups to S3 after dump ---
            # - name: s3-upload
            #   image: amazon/aws-cli:latest
            #   command:
            #     - /bin/sh
            #     - -c
            #     - |
            #       set -e
            #       echo "Waiting for mongodump to finish..."
            #       while [ ! -f /backups/.dump-complete ]; do sleep 5; done
            #
            #       LATEST=$(ls -t /backups/mongodb_*.gz | head -1)
            #       echo "Uploading ${LATEST} to s3://${S3_BUCKET}/mongodb/"
            #       aws s3 cp "${LATEST}" "s3://${S3_BUCKET}/mongodb/"
            #
            #       echo "Cleaning S3 backups older than 30 days..."
            #       aws s3 ls "s3://${S3_BUCKET}/mongodb/" | \
            #         awk '{print $4}' | while read file; do
            #           FILE_DATE=$(echo "$file" | grep -oP '\d{8}')
            #           CUTOFF=$(date -d "-30 days" +%Y%m%d)
            #           if [ "$FILE_DATE" -lt "$CUTOFF" ] 2>/dev/null; then
            #             aws s3 rm "s3://${S3_BUCKET}/mongodb/${file}"
            #           fi
            #         done
            #
            #       echo "S3 upload complete"
            #   env:
            #     - name: S3_BUCKET
            #       value: "your-bucket-name"
            #     - name: AWS_ACCESS_KEY_ID
            #       valueFrom:
            #         secretKeyRef:
            #           name: aws-s3-credentials
            #           key: accessKey
            #     - name: AWS_SECRET_ACCESS_KEY
            #       valueFrom:
            #         secretKeyRef:
            #           name: aws-s3-credentials
            #           key: secretKey
            #     - name: AWS_DEFAULT_REGION
            #       value: "us-east-1"
            #   volumeMounts:
            #     - name: backup-storage
            #       mountPath: /backups

          volumes:
            - name: backup-storage
              persistentVolumeClaim:
                claimName: mongodb-backups

# --- Uncomment to create AWS S3 credentials secret ---
# ---
# apiVersion: v1
# kind: Secret
# metadata:
#   name: aws-s3-credentials
#   namespace: databases
# type: Opaque
# stringData:
#   accessKey: "YOUR_AWS_ACCESS_KEY_ID"
#   secretKey: "YOUR_AWS_SECRET_ACCESS_KEY"
